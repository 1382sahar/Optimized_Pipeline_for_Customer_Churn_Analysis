{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install Required Libraries"
      ],
      "metadata": {
        "id": "xs87FZ_ySz0R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NR-ozx8Svjx"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install shap xgboost seaborn scikit-optimize catboost lightgbm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries"
      ],
      "metadata": {
        "id": "saP_wfxIS-mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "from skopt import BayesSearchCV"
      ],
      "metadata": {
        "id": "dXhvnMMdS-So"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and Preprocess Data"
      ],
      "metadata": {
        "id": "yIjeB0frTEeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 3: Load and Preprocess Data ---\n",
        "data = pd.read_csv('./Telco Customer Churn.csv')\n",
        "data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "for col in data.columns:\n",
        "    if data[col].dtype == 'object':\n",
        "        data[col] = label_encoder.fit_transform(data[col].astype(str))\n",
        "\n",
        "X = data.drop('Churn', axis=1)\n",
        "y = data['Churn']\n"
      ],
      "metadata": {
        "id": "dGMIVxyGTD-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split Data and Define Cross-Validation\n"
      ],
      "metadata": {
        "id": "GHNZ_bArTNAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 4: Split Data and Define Cross-Validation ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n"
      ],
      "metadata": {
        "id": "00TIJCaWTPsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Models and Hyperparameter Search Spaces"
      ],
      "metadata": {
        "id": "JYh8XGURTSmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 5: Define Models and Hyperparameter Search Spaces ---\n",
        "models = {\n",
        "    'Logistic Regression': {\n",
        "        'model': LogisticRegression(max_iter=1000, random_state=42),\n",
        "        'params': {\n",
        "            'feature_selection__k': (5, X_train.shape[1]),\n",
        "            'classifier__C': (1e-3, 1e3, 'log-uniform'),\n",
        "            'classifier__penalty': ['l2'],\n",
        "            'classifier__solver': ['lbfgs']\n",
        "        }\n",
        "    },\n",
        "\n",
        "    'Random Forest': {\n",
        "        'model': RandomForestClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'feature_selection__k': (5, X_train.shape[1]),\n",
        "            'classifier__n_estimators': (50, 300),\n",
        "            'classifier__max_depth': (3, 30),\n",
        "            'classifier__min_samples_split': (2, 20),\n",
        "            'classifier__min_samples_leaf': (1, 20),\n",
        "            'classifier__max_features': ['sqrt', 'log2']\n",
        "        }\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'model': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
        "        'params': {\n",
        "            'feature_selection__k': (5, X_train.shape[1]),\n",
        "            'classifier__n_estimators': (50, 300),\n",
        "            'classifier__max_depth': (3, 30),\n",
        "            'classifier__learning_rate': (0.01, 0.3, 'log-uniform'),\n",
        "            'classifier__subsample': (0.5, 1.0),\n",
        "            'classifier__colsample_bytree': (0.5, 1.0)\n",
        "        }\n",
        "    },\n",
        "    'LightGBM': {\n",
        "        'model': lgb.LGBMClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'feature_selection__k': (5, X_train.shape[1]),\n",
        "            'classifier__n_estimators': (50, 300),\n",
        "            'classifier__max_depth': (3, 30),\n",
        "            'classifier__learning_rate': (0.01, 0.3, 'log-uniform'),\n",
        "            'classifier__num_leaves': (20, 50),\n",
        "            'classifier__subsample': (0.5, 1.0),\n",
        "            'classifier__colsample_bytree': (0.5, 1.0)\n",
        "        }\n",
        "    },\n",
        "    'CatBoost': {\n",
        "        'model': CatBoostClassifier(verbose=0, random_seed=42),\n",
        "        'params': {\n",
        "            'feature_selection__k': (5, X_train.shape[1]),\n",
        "            'classifier__iterations': (50, 300),\n",
        "            'classifier__depth': (3, 10),\n",
        "            'classifier__learning_rate': (0.01, 0.3, 'log-uniform'),\n",
        "            'classifier__l2_leaf_reg': (1, 10)\n",
        "        }\n",
        "    },\n",
        "    'AdaBoost': {\n",
        "        'model': AdaBoostClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'feature_selection__k': (5, X_train.shape[1]),\n",
        "            'classifier__n_estimators': (50, 300),\n",
        "            'classifier__learning_rate': (0.01, 1.0, 'log-uniform')\n",
        "        }\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "NJbZ7TBcTSTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Models, Optimize, and Save Results"
      ],
      "metadata": {
        "id": "hlBmCG00TXhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 6: Train Models, Optimize, and Save Results ---\n",
        "results = []\n",
        "best_estimators = {}\n",
        "\n",
        "for name, m in models.items():\n",
        "    print(f\"--- {name} ---\")\n",
        "    # Base model without optimization\n",
        "    pipe_base = Pipeline([\n",
        "        ('feature_selection', SelectKBest(score_func=f_classif, k='all')),\n",
        "        ('classifier', m['model'])\n",
        "    ])\n",
        "    pipe_base.fit(X_train, y_train)\n",
        "    y_pred_base = pipe_base.predict(X_test)\n",
        "    acc_base = accuracy_score(y_test, y_pred_base)\n",
        "    print(f\"Base Accuracy: {acc_base:.4f}\")\n",
        "\n",
        "    # Model with Bayesian optimization\n",
        "    pipe_opt = Pipeline([\n",
        "        ('feature_selection', SelectKBest(score_func=f_classif)),\n",
        "        ('classifier', m['model'])\n",
        "    ])\n",
        "    opt = BayesSearchCV(\n",
        "        estimator=pipe_opt,\n",
        "        search_spaces=m['params'],\n",
        "        n_iter=30,\n",
        "        scoring='accuracy',\n",
        "        cv=cv,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    opt.fit(X_train, y_train)\n",
        "    y_pred_opt = opt.predict(X_test)\n",
        "    acc_opt = accuracy_score(y_test, y_pred_opt)\n",
        "    prec_opt = precision_score(y_test, y_pred_opt)\n",
        "    rec_opt = recall_score(y_test, y_pred_opt)\n",
        "    f1_opt = f1_score(y_test, y_pred_opt)\n",
        "\n",
        "    print(f\"Optimized Accuracy: {acc_opt:.4f}\")\n",
        "    print(f\"Optimized Precision: {prec_opt:.4f}\")\n",
        "    print(f\"Optimized Recall: {rec_opt:.4f}\")\n",
        "    print(f\"Optimized F1-Score: {f1_opt:.4f}\")\n",
        "    print(\"Best params:\", opt.best_params_)\n",
        "    print()\n",
        "\n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Base Accuracy': acc_base,\n",
        "        'Optimized Accuracy': acc_opt,\n",
        "        'Optimized Precision': prec_opt,\n",
        "        'Optimized Recall': rec_opt,\n",
        "        'Optimized F1-Score': f1_opt\n",
        "    })\n",
        "\n",
        "    best_estimators[name] = opt.best_estimator_\n",
        "\n",
        "results_df = pd.DataFrame(results)\n"
      ],
      "metadata": {
        "id": "le6tdPRlTaQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display Results Table"
      ],
      "metadata": {
        "id": "P_R2Zt1pTfcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 7: Display Results Table ---\n",
        "results_df.style.background_gradient(cmap='viridis')\n"
      ],
      "metadata": {
        "id": "9I4WKUzfTgNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Accuracy Comparison Before and After Optimization"
      ],
      "metadata": {
        "id": "03ocJbvxTiAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 8: Plot Accuracy Comparison ---\n",
        "plt.figure(figsize=(14, 7))\n",
        "bar_width = 0.35\n",
        "index = np.arange(len(results_df))\n",
        "\n",
        "plt.bar(index, results_df['Base Accuracy'], bar_width, label='Base Accuracy', alpha=0.7)\n",
        "plt.bar(index + bar_width, results_df['Optimized Accuracy'], bar_width, label='Optimized Accuracy', alpha=0.7)\n",
        "\n",
        "plt.xticks(index + bar_width / 2, results_df['Model'], rotation=45)\n",
        "plt.ylim(0, 1)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Comparison of Model Accuracies Before and After Bayesian Optimization')\n",
        "plt.legend()\n",
        "\n",
        "for i in range(len(results_df)):\n",
        "    plt.text(i, results_df['Base Accuracy'][i] + 0.02, f\"{results_df['Base Accuracy'][i]:.4f}\", ha='center')\n",
        "    plt.text(i + bar_width, results_df['Optimized Accuracy'][i] + 0.02, f\"{results_df['Optimized Accuracy'][i]:.4f}\", ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "E5ixjMwwTkcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot ROC Curves for Optimized Models"
      ],
      "metadata": {
        "id": "vDnk1R_fTnWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 9: Plot ROC Curves for Optimized Models ---\n",
        "plt.figure(figsize=(10, 8))\n",
        "for name, estimator in best_estimators.items():\n",
        "    if hasattr(estimator.named_steps['classifier'], \"predict_proba\"):\n",
        "        y_proba = estimator.predict_proba(X_test)[:, 1]\n",
        "    else:\n",
        "        y_score = estimator.decision_function(X_test)\n",
        "        y_proba = (y_score - y_score.min()) / (y_score.max() - y_score.min())  # Normalize to [0,1]\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.3f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for Optimized Models')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cigK2QTNTsYA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}